{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc5d128-7534-4b57-b32a-27b1153587cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting promptflow\n  Using cached promptflow-0.1.0b8-py3-none-any.whl (1.3 MB)\nCollecting promptflow-tools\n  Using cached promptflow_tools-0.1.0b11-py3-none-any.whl (27 kB)\nCollecting keyrings.alt\n  Using cached keyrings.alt-5.0.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: flask<3.0.0,>=2.2.3 in /databricks/python3/lib/python3.10/site-packages (from promptflow) (2.2.5)\nCollecting pyarrow<14.0.0,>=13.0.0\n  Using cached pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\nRequirement already satisfied: gitpython<4.0.0,>=3.1.24 in /databricks/python3/lib/python3.10/site-packages (from promptflow) (3.1.27)\nCollecting keyring<25.0.0,>=24.2.0\n  Using cached keyring-24.2.0-py3-none-any.whl (37 kB)\nCollecting waitress<3.0.0,>=2.1.2\n  Using cached waitress-2.1.2-py3-none-any.whl (57 kB)\nCollecting pillow<11.0.0,>=10.1.0\n  Using cached Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\nCollecting strictyaml<2.0.0,>=1.5.0\n  Using cached strictyaml-1.7.3-py3-none-any.whl (123 kB)\nRequirement already satisfied: filelock<4.0.0,>=3.4.0 in /databricks/python3/lib/python3.10/site-packages (from promptflow) (3.9.0)\nCollecting pydash<7.0.0,>=6.0.0\n  Using cached pydash-6.0.2-py3-none-any.whl (85 kB)\nRequirement already satisfied: tiktoken>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from promptflow) (0.5.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.5 in /databricks/python3/lib/python3.10/site-packages (from promptflow) (3.20.1)\nCollecting tabulate<1.0.0,>=0.9.0\n  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\nCollecting colorama<0.5.0,>=0.4.6\n  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nRequirement already satisfied: pandas<3.0.0,>=1.5.3 in /databricks/python3/lib/python3.10/site-packages (from promptflow) (1.5.3)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from promptflow) (5.9.0)\nCollecting ruamel.yaml<0.18.0,>=0.17.35\n  Using cached ruamel.yaml-0.17.40-py3-none-any.whl (113 kB)\nRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /databricks/python3/lib/python3.10/site-packages (from promptflow) (6.0)\nCollecting opencensus-ext-azure<2.0.0\n  Using cached opencensus_ext_azure-1.1.11-py2.py3-none-any.whl (43 kB)\nCollecting openai<0.28.0,>=0.27.8\n  Using cached openai-0.27.10-py3-none-any.whl (76 kB)\nCollecting dataset<2.0.0,>=1.6.0\n  Using cached dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nCollecting sqlalchemy<2.0.0,>=1.4.48\n  Using cached SQLAlchemy-1.4.50-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting cryptography<42.0.0,>=41.0.3\n  Using cached cryptography-41.0.5-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\nCollecting python-dotenv<2.0.0,>=1.0.0\n  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nCollecting google-search-results==2.4.1\n  Using cached google_search_results-2.4.1-py3-none-any.whl\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from google-search-results==2.4.1->promptflow-tools) (2.28.1)\nCollecting jaraco.classes\n  Using cached jaraco.classes-3.3.0-py3-none-any.whl (5.9 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography<42.0.0,>=41.0.3->promptflow) (1.15.1)\nCollecting alembic>=0.6.2\n  Using cached alembic-1.12.1-py3-none-any.whl (226 kB)\nCollecting banal>=1.0.1\n  Using cached banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.10/site-packages (from flask<3.0.0,>=2.2.3->promptflow) (2.0.1)\nRequirement already satisfied: Jinja2>=3.0 in /databricks/python3/lib/python3.10/site-packages (from flask<3.0.0,>=2.2.3->promptflow) (3.1.2)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from flask<3.0.0,>=2.2.3->promptflow) (2.2.2)\nRequirement already satisfied: click>=8.0 in /databricks/python3/lib/python3.10/site-packages (from flask<3.0.0,>=2.2.3->promptflow) (8.0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4.0.0,>=3.1.24->promptflow) (4.0.11)\nRequirement already satisfied: SecretStorage>=3.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.2.0->promptflow) (3.3.1)\nCollecting importlib-metadata>=4.11.4\n  Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: jeepney>=0.4.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.2.0->promptflow) (0.7.1)\nRequirement already satisfied: packaging>=17.0 in /databricks/python3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.5->promptflow) (22.0)\nRequirement already satisfied: aiohttp in /databricks/python3/lib/python3.10/site-packages (from openai<0.28.0,>=0.27.8->promptflow) (3.8.6)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from openai<0.28.0,>=0.27.8->promptflow) (4.64.1)\nCollecting opencensus<1.0.0,>=0.11.3\n  Using cached opencensus-0.11.3-py2.py3-none-any.whl (128 kB)\nRequirement already satisfied: azure-core<2.0.0,>=1.12.0 in /databricks/python3/lib/python3.10/site-packages (from opencensus-ext-azure<2.0.0->promptflow) (1.29.1)\nCollecting azure-identity<2.0.0,>=1.5.0\n  Using cached azure_identity-1.15.0-py3-none-any.whl (164 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=1.5.3->promptflow) (2022.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=1.5.3->promptflow) (2.8.2)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=1.5.3->promptflow) (1.23.5)\nCollecting ruamel.yaml.clib>=0.2.7\n  Using cached ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy<2.0.0,>=1.4.48->promptflow) (2.0.1)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.10/site-packages (from tiktoken>=0.4.0->promptflow) (2022.7.9)\nRequirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from jaraco.classes->keyrings.alt) (8.10.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic>=0.6.2->dataset<2.0.0,>=1.6.0->promptflow) (1.2.0)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.10/site-packages (from alembic>=0.6.2->dataset<2.0.0,>=1.6.0->promptflow) (4.4.0)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.12.0->opencensus-ext-azure<2.0.0->promptflow) (1.16.0)\nCollecting msal-extensions<2.0.0,>=0.3.0\n  Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\nCollecting msal<2.0.0,>=1.24.0\n  Using cached msal-1.25.0-py2.py3-none-any.whl (97 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.3->promptflow) (2.21)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.24->promptflow) (5.0.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.2.0->promptflow) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2>=3.0->flask<3.0.0,>=2.2.3->promptflow) (2.1.1)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (2.12.0)\nCollecting opencensus-context>=0.1.3\n  Using cached opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->google-search-results==2.4.1->promptflow-tools) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->google-search-results==2.4.1->promptflow-tools) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->google-search-results==2.4.1->promptflow-tools) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->google-search-results==2.4.1->promptflow-tools) (3.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai<0.28.0,>=0.27.8->promptflow) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai<0.28.0,>=0.27.8->promptflow) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai<0.28.0,>=0.27.8->promptflow) (22.1.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai<0.28.0,>=0.27.8->promptflow) (1.3.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai<0.28.0,>=0.27.8->promptflow) (1.9.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai<0.28.0,>=0.27.8->promptflow) (4.0.3)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (1.61.0)\nRequirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /databricks/python3/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (2.21.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /databricks/python3/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (4.24.0)\nRequirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/lib/python3/dist-packages (from msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->promptflow) (2.3.0)\nCollecting portalocker<3,>=1.0\n  Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (5.3.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (4.9)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.3->opencensus-ext-azure<2.0.0->promptflow) (0.4.8)\nInstalling collected packages: opencensus-context, banal, waitress, tabulate, sqlalchemy, ruamel.yaml.clib, python-dotenv, pydash, pyarrow, portalocker, pillow, jaraco.classes, importlib-metadata, colorama, strictyaml, ruamel.yaml, keyrings.alt, keyring, google-search-results, cryptography, alembic, openai, dataset, opencensus, msal, msal-extensions, azure-identity, opencensus-ext-azure, promptflow, promptflow-tools\n  Attempting uninstall: tabulate\n    Found existing installation: tabulate 0.8.10\n    Not uninstalling tabulate at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'tabulate'. No files were found to uninstall.\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 1.4.39\n    Not uninstalling sqlalchemy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'SQLAlchemy'. No files were found to uninstall.\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 8.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\n  Attempting uninstall: pillow\n    Found existing installation: Pillow 9.4.0\n    Not uninstalling pillow at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'Pillow'. No files were found to uninstall.\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 4.11.3\n    Not uninstalling importlib-metadata at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\n  Attempting uninstall: keyring\n    Found existing installation: keyring 23.5.0\n    Not uninstalling keyring at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'keyring'. No files were found to uninstall.\n  Attempting uninstall: cryptography\n    Found existing installation: cryptography 39.0.1\n    Not uninstalling cryptography at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'cryptography'. No files were found to uninstall.\n  Attempting uninstall: openai\n    Found existing installation: openai 0.28.1\n    Not uninstalling openai at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a162131-ce96-4aa3-b81a-c4a6200e7e28\n    Can't uninstall 'openai'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.16.1 requires pyspark<4,>=3.1.2, which is not installed.\nSuccessfully installed alembic-1.12.1 azure-identity-1.15.0 banal-1.0.6 colorama-0.4.6 cryptography-41.0.5 dataset-1.6.2 google-search-results-2.4.1 importlib-metadata-6.8.0 jaraco.classes-3.3.0 keyring-24.2.0 keyrings.alt-5.0.0 msal-1.25.0 msal-extensions-1.0.0 openai-0.27.10 opencensus-0.11.3 opencensus-context-0.1.3 opencensus-ext-azure-1.1.11 pillow-10.1.0 portalocker-2.8.2 promptflow-0.1.0b8 promptflow-tools-0.1.0b11 pyarrow-13.0.0 pydash-6.0.2 python-dotenv-1.0.0 ruamel.yaml-0.17.40 ruamel.yaml.clib-0.2.8 sqlalchemy-1.4.50 strictyaml-1.7.3 tabulate-0.9.0 waitress-2.1.2\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "### Install pf requirements\n",
    "!pip install promptflow promptflow-tools keyrings.alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8799853-01d0-4da1-9f8f-cc3a280fd8f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578c3fb3-a87a-4f92-828f-3522f7543fec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Check to ensure promptflow is installed: !pf -v should display \"0.1.0b8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a18ed6a-2e40-4f90-8745-72d36fdda71c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0b8\r\n"
     ]
    }
   ],
   "source": [
    "!pf -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6f6afe0-8a76-4100-b6ad-f2b442a32680",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Start by checking promptflow will run locally, setup connections defined in the flow.dag.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a5b9e0-fa0c-4499-9099-dd2de64a4cf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created connection\nname: aoai-connection\nmodule: promptflow.connections\ncreated_date: '2023-11-09T05:08:52.212200'\nlast_modified_date: '2023-11-09T05:08:52.212200'\ntype: azure_open_ai\napi_key: '******'\napi_base: https://eu2-oai.openai.azure.com/\napi_type: azure\napi_version: 2023-07-01-preview\n\n"
     ]
    }
   ],
   "source": [
    "#Create pf connection\n",
    "\n",
    "from promptflow import PFClient\n",
    "from promptflow.entities import AzureOpenAIConnection\n",
    "\n",
    "# PFClient can help manage your runs and connections.\n",
    "pf = PFClient()\n",
    "\n",
    "try:\n",
    "    conn_name = \"aoai-connection\"\n",
    "    conn = pf.connections.get(name=conn_name)\n",
    "    print(\"using existing connection\")\n",
    "except:\n",
    "    #use keyvault to store secret keys\n",
    "    connection = AzureOpenAIConnection(\n",
    "        name=conn_name,\n",
    "        api_key=\"REPLACE_KEY\",\n",
    "        api_base=\"REPLACE_API_BASE\",\n",
    "        api_type=\"azure\",\n",
    "        api_version= \"2023-07-01-preview\",\n",
    "    )\n",
    "\n",
    "    conn = pf.connections.create_or_update(connection)\n",
    "    print(\"successfully created connection\")\n",
    "\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc69e036-b2ba-4a88-877a-b60d411ae230",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 05:08:55 +0000    1727 execution.flow     INFO     Start to run 6 nodes with concurrency level 16.\n2023-11-09 05:08:55 +0000    1727 execution.flow     INFO     Executing node llm_python. node run id: 4ad0726b-fa66-405e-828d-403c132b47c0_llm_python_0\n2023-11-09 05:08:56 +0000    1727 execution.flow     INFO     Node llm_python completes.\n2023-11-09 05:08:56 +0000    1727 execution.flow     INFO     Executing node analyze_df. node run id: 4ad0726b-fa66-405e-828d-403c132b47c0_analyze_df_0\n2023-11-09 05:08:56 +0000    1727 execution.flow     INFO     [analyze_df in line 0 (index starts from 0)] stdout> import pandas as pd\ndf = pd.read_csv('Mock_Count_index_Data_20230928G.csv')\n# Assuming your dataframe is named 'df'\nhighest_sales_index_states = df.groupby('geo')['sales_index'].max().sort_values(ascending=False)\nans = highest_sales_index_states\nprint(ans)\n2023-11-09 05:08:58 +0000    1727 execution.flow     INFO     [analyze_df in line 0 (index starts from 0)] stdout> CompletedProcess(args=['python', '-c', \"import pandas as pd\\ndf = pd.read_csv('Mock_Count_index_Data_20230928G.csv')\\n# Assuming your dataframe is named 'df'\\nhighest_sales_index_states = df.groupby('geo')['sales_index'].max().sort_values(ascending=False)\\nans = highest_sales_index_states\\nprint(ans)\"], returncode=0, stdout=b'geo\\nGA    199.235498\\nMN    199.168004\\nAK    198.894644\\nUS    197.693333\\nNV    189.449335\\nTX    189.068584\\nCA    179.230854\\nAL    175.068385\\nNY    173.397386\\nFL    171.483014\\nNJ    143.090332\\nName: sales_index, dtype: float64\\n', stderr=b'')\n2023-11-09 05:08:58 +0000    1727 execution.flow     INFO     Node analyze_df completes.\n2023-11-09 05:08:58 +0000    1727 execution.flow     INFO     Executing node output_format. node run id: 4ad0726b-fa66-405e-828d-403c132b47c0_output_format_0\n2023-11-09 05:08:58 +0000    1727 execution.flow     INFO     Executing node generate_insights. node run id: 4ad0726b-fa66-405e-828d-403c132b47c0_generate_insights_0\n2023-11-09 05:09:00 +0000    1727 execution.flow     INFO     Node output_format completes.\n2023-11-09 05:09:00 +0000    1727 execution.flow     INFO     Node generate_insights completes.\n2023-11-09 05:09:00 +0000    1727 execution.flow     INFO     Executing node insight_result. node run id: 4ad0726b-fa66-405e-828d-403c132b47c0_insight_result_0\n2023-11-09 05:09:00 +0000    1727 execution.flow     INFO     Node insight_result completes.\n2023-11-09 05:09:00 +0000    1727 execution.flow     INFO     Executing node concat_result. node run id: 4ad0726b-fa66-405e-828d-403c132b47c0_concat_result_0\n2023-11-09 05:09:00 +0000    1727 execution.flow     INFO     Node concat_result completes.\nProgram:\n```\n# Assuming your dataframe is named 'df'\nhighest_sales_index_states = df.groupby('geo')['sales_index'].max().sort_values(ascending=False)\nans = highest_sales_index_states\n```\n\nResult:\n```\ngeo | sales_index\n--- | ---\nGA | 199.235498\nMN | 199.168004\nAK | 198.894644\nUS | 197.693333\nNV | 189.449335\nTX | 189.068584\nCA | 179.230854\nAL | 175.068385\nNY | 173.397386\nFL | 171.483014\nNJ | 143.090332\n\nName: sales_index, dtype: float64\n```\n\nInsights:\n```\n```\ngeo | sales_index\n--- | ---\nGA | 199.235498\nMN | 199.168004\nAK | 198.894644\nUS | 197.693333\nNV | 189.449335\nTX | 189.068584\nCA | 179.230854\nAL | 175.068385\nNY | 173.397386\nFL | 171.483014\nNJ | 143.090332\n\nName: sales_index, dtype: float64\n```\nThe provided data seems to be a sales index for different geographical locations. The table includes the geographical code (e.g., GA, MN, AK) and the corresponding sales index values. Here are the top 3 locations with the highest sales index:\n\n1. Georgia (GA): 199.235498\n2. Minnesota (MN): 199.168004\n3. Alaska (AK): 198.894644\n\nPlease note that the data provided is limited, and I can only provide insights based on the information available. If you have any specific questions or need further analysis, please let me know.\n```\n"
     ]
    }
   ],
   "source": [
    "from promptflow import PFClient\n",
    "import os\n",
    "\n",
    "pf = PFClient()\n",
    "\n",
    "flow_path =  \"../chat_csv_model/promptflow\"  # \"web-classification\" is the directory name\n",
    "question = 'which states have the highest sales index?'\n",
    "# Test flow\n",
    "flow_inputs = {\"columnlist\": '''[ uid : identifier, geo : geography, naics_code_3 : industry code,\n",
    "          naics_code_2 : industry code, period : date format YYYYMMDD,\n",
    "          count_index : count_index, sales_index : sales_index, count_mom_diff :\n",
    "          count_mom_diff, count_mom_rank : count_mom_rank, count_yoy_diff :\n",
    "          count_yoy_diff, count_yoy_rank : count_yoy_rank,  sales_mom_diff :\n",
    "          sales_mom_diff, sales_mom_rank : sales_mom_rank, sales_yoy_diff :\n",
    "          sales_yoy_diff, sales_yoy_rank : sales_yoy_rank, count_index_adj :\n",
    "          count_index_adj, sales_index_adj :\n",
    "          sales_index_adj,  count_adj_mom_diff : count_adj_mom_diff,\n",
    "          count_adj_mom_rank : count_adj_mom_rank, count_adj_yoy_diff :\n",
    "          count_adj_yoy_diff, count_adj_yoy_rank : count_adj_yoy_rank,\n",
    "          sales_adj_mom_diff : sales_adj_mom_diff, sales_adj_mom_rank :\n",
    "          sales_adj_mom_rank, sales_adj_yoy_diff : sales_adj_yoy_diff,\n",
    "          sales_adj_yoy_rank : sales_adj_yoy_rank, partition_by : YYYYMM ]''',\n",
    "        'filename': 'Mock_Count_index_Data_20230928G.csv',\n",
    "        'question': question }  # The inputs of the flow.\n",
    "flow_result = pf.test(flow=flow_path, inputs=flow_inputs)\n",
    "print(flow_result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa6dcaca-b944-41b6-9c4c-686f446c8447",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Model Class to register as promptflow model.  Please note the promptflow expects model \"gpt-35-turbo\" to be deployed to endpoint\n",
    "\n",
    "###TODO: Replace connection key and api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fad1951-fa8d-4648-afe7-43f183a4d2f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Define the model class\n",
    "class chatcsvpf(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from promptflow import PFClient\n",
    "        from promptflow.entities import AzureOpenAIConnection\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        # PFClient can help manage your runs and connections.\n",
    "        pf = PFClient()\n",
    "\n",
    "        try:\n",
    "            conn_name = \"aoai-connection\"\n",
    "            conn = pf.connections.get(name=conn_name)\n",
    "            print(\"using existing connection\")\n",
    "        except:\n",
    "            #use keyvault to store secret keys\n",
    "            connection = AzureOpenAIConnection(\n",
    "                name=conn_name,\n",
    "                api_key=\"REPLACE_KEY\",\n",
    "                api_base=\"REPLACE_API_BASE\",\n",
    "                api_type=\"azure\",\n",
    "                api_version= \"2023-07-01-preview\",\n",
    "            )\n",
    "            conn = pf.connections.create_or_update(connection)      \n",
    "        self.flow_path = \"{}/promptflow\".format(context.artifacts[\"chatcsvpf\"])#\"promptflow\"  # \"web-classification\" is the directory name  \n",
    "\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        pf = PFClient()\n",
    "        question = model_input[\"question\"][0]\n",
    "        if isinstance(question, np.ndarray):\n",
    "            question = question.item()\n",
    "        cols = model_input[\"columnlist\"][0]\n",
    "        if isinstance(cols, np.ndarray):\n",
    "            cols = cols.item()\n",
    "        filename = model_input[\"filename\"][0]\n",
    "        if isinstance(filename, np.ndarray):\n",
    "            filename = filename.item()\n",
    "        # Test flow\n",
    "        flow_inputs = {\"columnlist\": cols,\n",
    "                        'filename': filename,\n",
    "                'question': question }  # The inputs of the flow.\n",
    "        flow_result = pf.test(flow=flow_path, inputs=flow_inputs)\n",
    "        return(flow_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82afd618-fd9f-410d-92e8-a1ec171d2243",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Register promptflow model to mlflow model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426e1aef-472e-4d59-9981-ca095d1bf67d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e05b9ab4d4436a86379839143f69e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/09 05:10:29 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2634f03d99454788ddf7871dacb5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/09 05:10:36 INFO mlflow.store.artifact.cloud_artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\nSuccessfully registered model 'chat_csv-promptflow'.\n2023/11/09 05:10:38 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: chat_csv-promptflow, version 1\nCreated version '1' of model 'chat_csv-promptflow'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    artifacts = {\"chatcsvpf\": \"../chat_csv_model\"}\n",
    "    modelpath = \"chat_csv-promptflow\"\n",
    "    sample_input = {\"columnlist\": '''[ uid : identifier, geo : geography, naics_code_3 : industry code,\n",
    "          naics_code_2 : industry code, period : date format YYYYMMDD,\n",
    "          count_index : count_index, sales_index : sales_index, count_mom_diff :\n",
    "          count_mom_diff, count_mom_rank : count_mom_rank, count_yoy_diff :\n",
    "          count_yoy_diff, count_yoy_rank : count_yoy_rank,  sales_mom_diff :\n",
    "          sales_mom_diff, sales_mom_rank : sales_mom_rank, sales_yoy_diff :\n",
    "          sales_yoy_diff, sales_yoy_rank : sales_yoy_rank, count_index_adj :\n",
    "          count_index_adj, sales_index_adj :\n",
    "          sales_index_adj,  count_adj_mom_diff : count_adj_mom_diff,\n",
    "          count_adj_mom_rank : count_adj_mom_rank, count_adj_yoy_diff :\n",
    "          count_adj_yoy_diff, count_adj_yoy_rank : count_adj_yoy_rank,\n",
    "          sales_adj_mom_diff : sales_adj_mom_diff, sales_adj_mom_rank :\n",
    "          sales_adj_mom_rank, sales_adj_yoy_diff : sales_adj_yoy_diff,\n",
    "          sales_adj_yoy_rank : sales_adj_yoy_rank, partition_by : YYYYMM ]''',\n",
    "        'filename': 'Mock_Count_index_Data_20230928G.csv',\n",
    "        'question': \"question\" }  # The inputs of the flow.\n",
    "    mlflow.pyfunc.log_model(\n",
    "        modelpath,\n",
    "        python_model=chatcsvpf(),\n",
    "        artifacts=artifacts,\n",
    "        signature=infer_signature(sample_input, [\"Run\"]),\n",
    "        extra_pip_requirements=[\"promptflow\", \"pandas\", \"promptflow-tools\", \"keyrings.alt\", \"numpy\"]\n",
    "    )\n",
    "    run_id = run.info.run_id\n",
    "    mlflow.register_model(f\"runs:/{run_id}/{modelpath}\", modelpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51f38fd0-d83e-44ca-a8b1-858646ab2291",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Go to Model Registry to confirm model has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "649c9148-daca-4202-bc1a-8fb493c7df5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pf_register_model.ipynb",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
