{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit to @wenhuchen -  https://github.com/wenhuchen/Program-of-Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai\n",
    "from datetime import datetime\n",
    "from tool import finqa_equal, parse_api_result, safe_execute\n",
    "from eval_tatqa.tatqa_utils import extract_one_num_from_str\n",
    "from typing import Dict, Any\n",
    "import argparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_reader_request_processed(example: Dict[str, Any]):\n",
    "    prompt = 'Read the following text and table, and then answer the last question in a series of questions:\\n'\n",
    "    if example['golden_text']:\n",
    "        prompt += example['golden_text'].strip() + '\\n'\n",
    "    if example['golden_table']:\n",
    "        prompt += example['golden_table'].strip() + '\\n'\n",
    "    #prompt += '\\n'\n",
    "    prompt += 'Questions: '\n",
    "    prompt += \" \".join(example['questions'][:-1])\n",
    "    prompt += '\\n'\n",
    "    prompt += f'Question: {example[\"questions\"][-1]}'\n",
    "    prompt += '\\n'\n",
    "    prompt += 'Answer:'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "prompt_4shot = \"\"\"Read the following text and table, and then answer the last question in a series of questions:\n",
    "- | shares available for awards | shares subject to outstanding awards\n",
    "2009 global incentive plan | 2322450 | 2530454\n",
    "2004 stock incentive plan | - | 5923147\n",
    "Questions: how many shares are subject to outstanding awards is under the 2009 global incentive plan? what about under the 2004 stock incentive plan? how many total shares are subject to outstanding awards? what about under the 2004 stock incentive plan?\n",
    "Question: what proportion does this represent?\n",
    "#Python\n",
    "shares_subject_to_outstanding_awards_2009_global_incentive_plan = 2530454\n",
    "shares_subject_to_outstanding_awards_2004_stock_incentive_plan = 5923147\n",
    "total_shares_subject_to_outstanding_awards = shares_subject_to_outstanding_awards_2009_global_incentive_plan + shares_subject_to_outstanding_awards_2004_stock_incentive_plan\n",
    "proportion = shares_subject_to_outstanding_awards_2009_global_incentive_plan / total_shares_subject_to_outstanding_awards\n",
    "ans = proportion\n",
    "\n",
    "\n",
    "Read the following text and table, and then answer the last question in a series of questions:\n",
    "compensation expense the company recorded $ 43 million , $ 34 million , and $ 44 million of expense related to stock awards for the years ended december 31 , 2015 , 2014 , and 2013 , respectively . \n",
    "Questions: what is the compensation expense the company recorded in 2015? what about in 2014? what is the total compensation expense the company recorded in 2015 and 2014? what is the total expenses including 2013?\n",
    "Question: what is the average for three years?\n",
    "#Python\n",
    "compensation_expense_2015 = 43\n",
    "compensation_expense_2014 = 34\n",
    "compensation_expense_2013 = 44\n",
    "total_compensation_expense = compensation_expense_2015 + compensation_expense_2014 + compensation_expense_2013\n",
    "average_for_three_years = total_compensation_expense / 3\n",
    "ans = average_for_three_years\n",
    "\n",
    "\n",
    "Read the following text and table, and then answer the last question in a series of questions:\n",
    "the net loss on disposal of those assets was $ 344000 for 2005 and $ 43000 for 2004 . \n",
    "Questions: what was the net loss on disposal of assets in 2005? what was the value in 2004? what was the change in value?\n",
    "Question: what was the percent change?\n",
    "#Python\n",
    "net_loss_on_disposal_of_assets_2005 = 344000\n",
    "net_loss_on_disposal_of_assets_2004 = 43000\n",
    "net_change_in_value = net_loss_on_disposal_of_assets_2005 - net_loss_on_disposal_of_assets_2004\n",
    "percent_change = net_change_in_value / net_loss_on_disposal_of_assets_2004\n",
    "ans = percent_change\n",
    "\n",
    "\n",
    "Read the following text and table, and then answer the last question in a series of questions:\n",
    "location | operations conducted | approximatesquare feet | leaseexpirationdates\n",
    "dublin ireland | global supply chain distribution and administration offices | 160000 | owned\n",
    "athlone ireland | commercial research and development manufacturing | 80000 | owned\n",
    "bogart georgia | commercial research and development manufacturing | 70000 | owned\n",
    "smithfield rhode island | commercial research and development manufacturing | 67000 | owned\n",
    "Questions: what is the square feet of the owned global supply chain distribution and administration offices? what is the square feet of the owned commercial research and development manufacturing? what is the sum of those values? what is the total sum including square feet of commercial research and development manufacturing in bogart, georgia? what is the total sum including square feet of commercial research and development manufacturing in smithfield, rhode island?\n",
    "Question: what is the total sum of square feet owned?\n",
    "#Python\n",
    "square_feet_owned = [160000, 80000, 70000, 67000]\n",
    "total_square_feet_owned = sum(square_feet_owned)\n",
    "ans = total_square_feet_owned\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from configparser import ConfigParser\n",
    "\n",
    "parser=ConfigParser()\n",
    "_=parser.read('../../config.cfg')\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = parser.get('openai_api','api_ep')\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = parser.get('openai_api','api_key')\n",
    "model =  parser.get('openai_api','api_model')\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]  = openai.api_key\n",
    "os.environ[\"OPENAI_API_TYPE\"] = openai.api_type\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai.api_version\n",
    "os.environ[\"OPENAI_API_BASE\"] = openai.api_base\n",
    "model_name = \"davinci3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/convfinqa_dev.json') as f:\n",
    "        convfinqa_dev = json.load(f)\n",
    "    \n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%m_%d_%H_%M\")\n",
    "\n",
    "correct, wrong = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'outputs/convfinqa_direct_{model}_{dt_string}.jsonl'\n",
    "writer = open(filename, 'w')\n",
    "\n",
    "for example in tqdm(convfinqa_dev):\n",
    "    full_prompt = prompt_4shot + \"\\n\\n\"\n",
    "    full_prompt += create_reader_request_processed(example)\n",
    "    # greedy decoding\n",
    "    got_result = False\n",
    "    while not got_result:\n",
    "        try:\n",
    "            result = openai.Completion.create(\n",
    "                engine=model,\n",
    "                prompt=full_prompt,\n",
    "                max_tokens=256,\n",
    "                temperature=0.0,\n",
    "                top_p=1,\n",
    "                n=1,\n",
    "                stop=['\\n\\n'],\n",
    "                logprobs=1\n",
    "            )\n",
    "            got_result = True\n",
    "        except Exception:\n",
    "            sleep(3)\n",
    "    result_counter = Counter()\n",
    "    codes = parse_api_result(result)\n",
    "\n",
    "    for r in codes:\n",
    "        try :\n",
    "            exec(r)#extract_one_num_from_str(r)\n",
    "        except:\n",
    "            ans=r\n",
    "        if not ans:\n",
    "            if 'yes' in r.lower() or 'true' in r.lower():\n",
    "                ans = 'yes'\n",
    "            elif 'no' in r.lower() or 'false' in r.lower():\n",
    "                ans = 'no'\n",
    "        if ans is not None:\n",
    "            if type(ans) in [dict]:\n",
    "                result_counter.update(list(ans.values()))\n",
    "            elif type(ans) in [list, tuple]:\n",
    "                result_counter.update([float(ans[0])])\n",
    "            elif type(ans) in [str]:\n",
    "                result_counter.update([ans])\n",
    "            else:\n",
    "                try:\n",
    "                    result_counter.update([float(ans)])\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    if len(result_counter) > 0:\n",
    "        prediction = result_counter.most_common(1)[0][0]        \n",
    "    else:\n",
    "        prediction = None\n",
    "\n",
    "    if prediction is None:\n",
    "        wrong += 1\n",
    "    elif finqa_equal(prediction, example['answer'], True, True):\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "    example.update({'generated': codes, 'executed': prediction})\n",
    "    writer.write(json.dumps(example) + '\\n')\n",
    "    print('accuracy: ', correct / (correct + wrong))\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6247030878859857\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: ', correct / (correct + wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "gradio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
